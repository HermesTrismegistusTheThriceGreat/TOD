# Architectural Review Report

**Generated**: 2026-01-10T12:30:00Z
**Reviewed Work**: Alpaca Trading API Integration for IronCondorCard - Architecture Plan
**Plan Reference**: `/Users/muzz/Desktop/tac/TOD/specs/alpaca-ironcondor-integration-plan.md`
**Review Type**: Architecture Review (Pre-Implementation)
**Verdict**: ‚úÖ PASS (with recommendations)

---

## Executive Summary

The proposed architecture for integrating Alpaca Trading API with the IronCondorCard component is **well-designed** and follows a solid hybrid REST + WebSocket pattern that aligns with existing codebase conventions. The plan demonstrates good separation of concerns, proper use of Pydantic models, and a clear data flow from external API to frontend components. However, there are several architectural improvements that would enhance **scalability, resilience, and maintainability**.

---

## Quick Reference

| #   | Description                              | Risk Level | Recommended Solution                 |
| --- | ---------------------------------------- | ---------- | ------------------------------------ |
| 1   | No circuit breaker for Alpaca API        | HIGH       | Add circuit breaker pattern          |
| 2   | Missing rate limiting for price streams  | HIGH       | Add throttling/debouncing            |
| 3   | Global singleton for AlpacaService       | MEDIUM     | Consider dependency injection        |
| 4   | No backpressure handling for WebSocket   | MEDIUM     | Add queue-based broadcasting         |
| 5   | Price cache unbounded growth             | MEDIUM     | Add TTL-based cache eviction         |
| 6   | Blocking run_in_executor for sync calls  | MEDIUM     | Consider async Alpaca client         |
| 7   | Missing heartbeat for Alpaca WebSocket   | LOW        | Add health check mechanism           |
| 8   | No retry logic for initial fetch         | LOW        | Add exponential backoff              |
| 9   | snake_case/camelCase transform overhead  | LOW        | Consider consistent naming           |

---

## Issues by Risk Tier

### ‚ö†Ô∏è HIGH RISK (Should Fix Before Merge)

#### Issue #1: No Circuit Breaker Pattern for Alpaca API Calls

**Description**: The AlpacaService directly calls Alpaca APIs without any circuit breaker pattern. If Alpaca API becomes unavailable or slow, the backend will continue making failed requests, potentially blocking resources and degrading overall system performance.

**Location**:
- File: `specs/alpaca-ironcondor-integration-plan.md`
- Lines: `601-636` (get_all_positions method)

**Proposed Code**:
```python
async def get_all_positions(self) -> List[IronCondorPosition]:
    try:
        client = self._get_trading_client()
        loop = asyncio.get_event_loop()
        positions = await loop.run_in_executor(
            None, client.get_all_positions
        )
        # No circuit breaker - repeated failures will exhaust resources
```

**Recommended Solutions**:
1. **Add Circuit Breaker Pattern** (Preferred)
   - Use `tenacity` or `circuitbreaker` library
   - Track failure rate and open circuit when threshold exceeded
   - Rationale: Industry-standard pattern for external API resilience

   ```python
   from circuitbreaker import circuit

   @circuit(failure_threshold=5, recovery_timeout=30)
   async def get_all_positions(self) -> List[IronCondorPosition]:
       # existing implementation
   ```

2. **Add Health Check State**
   - Track API health status manually
   - Short-circuit calls when API known to be down
   - Trade-off: More manual implementation, less robust

---

#### Issue #2: No Rate Limiting or Throttling for Price Stream Updates

**Description**: The plan broadcasts every quote update directly to all WebSocket clients. With high-frequency option data (potentially hundreds of updates per second during market hours), this could:
1. Overwhelm frontend clients with too many updates
2. Create excessive CPU usage for JSON serialization
3. Cause browser performance issues

**Location**:
- File: `specs/alpaca-ironcondor-integration-plan.md`
- Lines: `766-798` (_handle_quote_update method)

**Proposed Code**:
```python
async def _handle_quote_update(self, quote) -> None:
    # Every quote immediately broadcasts to all clients
    await ws_manager.broadcast_option_price_update(update.model_dump())
```

**Recommended Solutions**:
1. **Add Throttling/Debouncing** (Preferred)
   - Batch updates per symbol with 100-500ms windows
   - Only send if price changed significantly (e.g., > 0.01)
   - Rationale: Reduces network traffic while maintaining responsiveness

   ```python
   class ThrottledPriceUpdater:
       def __init__(self, min_interval_ms: int = 250):
           self._last_update: Dict[str, float] = {}
           self._pending_updates: Dict[str, OptionPriceUpdate] = {}

       async def queue_update(self, update: OptionPriceUpdate):
           now = time.time()
           last = self._last_update.get(update.symbol, 0)
           if now - last >= self.min_interval_ms / 1000:
               await self._broadcast(update)
               self._last_update[update.symbol] = now
           else:
               self._pending_updates[update.symbol] = update
   ```

2. **Client-Side Throttling**
   - Let frontend handle throttling via composable
   - Trade-off: Still sends all updates over network

---

### ‚ö° MEDIUM RISK (Fix Soon)

#### Issue #3: Global Singleton Pattern for AlpacaService

**Description**: The plan uses a global singleton pattern (`_alpaca_service = None` + `get_alpaca_service()`) which makes testing difficult and creates tight coupling.

**Location**:
- File: `specs/alpaca-ironcondor-integration-plan.md`
- Lines: `836-854`

**Proposed Code**:
```python
# Global service instance
_alpaca_service: Optional[AlpacaService] = None

def get_alpaca_service() -> AlpacaService:
    global _alpaca_service
    if _alpaca_service is None:
        _alpaca_service = AlpacaService()
    return _alpaca_service
```

**Recommended Solutions**:
1. **Use Dependency Injection via app.state** (Preferred)
   - Already done for `orchestrator_service` and `autocomplete_service`
   - Store service in `app.state.alpaca_service`
   - Rationale: Consistent with existing patterns, easier testing

   ```python
   # In lifespan:
   app.state.alpaca_service = AlpacaService()

   # In endpoints:
   alpaca_service = request.app.state.alpaca_service
   ```

2. **Keep Singleton but Add Factory Reset**
   - Add `reset_alpaca_service()` for testing
   - Trade-off: Still has global state issues

---

#### Issue #4: No Backpressure Handling for WebSocket Broadcasting

**Description**: The existing `WebSocketManager.broadcast()` method iterates through all connections and sends synchronously. With many clients and high-frequency updates, slow clients could block the broadcast loop.

**Location**:
- File: `apps/orchestrator_3_stream/backend/modules/websocket_manager.py`
- Lines: `82-119` (existing broadcast method)
- Also: Proposed `broadcast_option_price_update` at line 878-890 of plan

**Current Code**:
```python
for connection in self.active_connections:
    try:
        await connection.send_json(data)  # Can block on slow client
    except Exception as e:
        disconnected.append(connection)
```

**Recommended Solutions**:
1. **Add Async Queue per Client** (Preferred)
   - Each client has a bounded queue
   - Background task drains queue per client
   - Drop oldest messages when queue full
   - Rationale: Prevents slow clients from blocking broadcast

   ```python
   class ConnectionWithQueue:
       websocket: WebSocket
       queue: asyncio.Queue[dict]  # maxsize=100

   async def broadcast(self, data: dict):
       for conn in self.active_connections:
           try:
               conn.queue.put_nowait(data)  # Non-blocking
           except asyncio.QueueFull:
               # Drop oldest message for slow client
               conn.queue.get_nowait()
               conn.queue.put_nowait(data)
   ```

2. **Use Fire-and-Forget with Timeout**
   - Add timeout to send_json calls
   - Trade-off: May lose messages to slow clients

---

#### Issue #5: Unbounded Price Cache Growth

**Description**: The `_price_cache` dictionary in AlpacaService grows indefinitely. For long-running sessions or many symbols, memory usage could grow significantly.

**Location**:
- File: `specs/alpaca-ironcondor-integration-plan.md`
- Lines: `558, 789`

**Proposed Code**:
```python
self._price_cache: Dict[str, OptionPriceUpdate] = {}
# ...
self._price_cache[symbol] = update  # Never evicted
```

**Recommended Solutions**:
1. **Add TTL-Based Eviction** (Preferred)
   - Use `cachetools.TTLCache` or similar
   - Evict prices older than 5-10 minutes
   - Rationale: Prevents memory leaks in long-running sessions

   ```python
   from cachetools import TTLCache

   self._price_cache: TTLCache[str, OptionPriceUpdate] = TTLCache(
       maxsize=10000, ttl=600  # 10 minutes
   )
   ```

2. **Clear Cache on Session End**
   - Clear when last subscriber disconnects
   - Trade-off: May re-fetch stale prices on reconnect

---

#### Issue #6: Blocking Executor for Sync Alpaca Client

**Description**: The plan uses `run_in_executor(None, client.get_all_positions)` which uses the default thread pool. This is acceptable but not ideal for a potentially slow external API call.

**Location**:
- File: `specs/alpaca-ironcondor-integration-plan.md`
- Lines: `611-615`

**Proposed Code**:
```python
loop = asyncio.get_event_loop()
positions = await loop.run_in_executor(
    None, client.get_all_positions  # Uses default executor
)
```

**Recommended Solutions**:
1. **Use Dedicated ThreadPoolExecutor** (Preferred)
   - Separate thread pool for Alpaca API calls
   - Prevents blocking of other async tasks
   - Rationale: Isolates slow external calls

   ```python
   from concurrent.futures import ThreadPoolExecutor

   self._alpaca_executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="alpaca_")

   positions = await loop.run_in_executor(
       self._alpaca_executor, client.get_all_positions
   )
   ```

2. **Check for Async Alpaca Client**
   - alpaca-py may have async methods
   - Trade-off: May require API changes

---

### üí° LOW RISK (Nice to Have)

#### Issue #7: Missing Heartbeat for Alpaca WebSocket Stream

**Description**: The OptionDataStream connection doesn't have explicit heartbeat handling in the plan. If the connection silently fails, no updates will flow until manually restarted.

**Location**:
- File: `specs/alpaca-ironcondor-integration-plan.md`
- Lines: `756-764` (_run_stream method)

**Recommended Solutions**:
1. **Add Watchdog Timer**
   - Track last update timestamp per symbol
   - If no updates for 60+ seconds during market hours, reconnect
   - Rationale: Ensures stream reliability

---

#### Issue #8: No Retry Logic for Initial Position Fetch

**Description**: The initial `get_all_positions()` call has no retry logic. A transient network issue could cause the feature to fail on startup.

**Location**:
- File: `specs/alpaca-ironcondor-integration-plan.md`
- Lines: `601-636`

**Recommended Solutions**:
1. **Add Exponential Backoff**
   - Use `tenacity` library with 3 retries
   - Exponential backoff: 1s, 2s, 4s
   - Rationale: Handles transient failures gracefully

---

#### Issue #9: Data Transform Overhead Between Backend and Frontend

**Description**: The plan requires snake_case to camelCase transformation on every position and price update. While necessary for JS conventions, this adds processing overhead.

**Location**:
- File: `specs/alpaca-ironcondor-integration-plan.md`
- Lines: `1199-1239` (transform functions)

**Recommended Solutions**:
1. **Consider Pydantic Alias Generation**
   - Use `model_config = ConfigDict(alias_generator=to_camel)` in Pydantic models
   - Backend sends camelCase directly
   - Trade-off: Slightly more complex backend models

---

## Architecture Analysis

### Overall Architecture: ‚úÖ Well-Designed

The hybrid REST + WebSocket approach is appropriate for this use case:
- **REST for Position Data**: One-time fetch with caching is correct for relatively static position structure
- **WebSocket for Prices**: Real-time streaming is essential for live P/L updates
- **Separation of Concerns**: Clear boundaries between service, models, and endpoints

### Data Flow: ‚úÖ Clear and Correct

```
Alpaca API ‚Üí AlpacaService ‚Üí REST/WebSocket ‚Üí Frontend Composables ‚Üí Vue Component
```

The data flow is linear and well-documented. The plan correctly:
- Centralizes Alpaca SDK calls in AlpacaService
- Uses WebSocketManager for broadcasting (consistent with existing patterns)
- Creates composables for component-level state management

### Component Separation: ‚úÖ Good

| Layer | Component | Responsibility |
|-------|-----------|----------------|
| External | Alpaca SDK | API communication |
| Service | AlpacaService | Business logic, caching, stream management |
| API | FastAPI endpoints | Request/response handling |
| Transport | WebSocketManager | Real-time broadcasting |
| Frontend | alpacaService.ts | REST client |
| Frontend | useAlpacaPositions | Position state management |
| Frontend | useAlpacaPriceStream | Price update handling |
| Component | IronCondorCard | UI rendering |

### Scalability Considerations: ‚ö†Ô∏è Needs Improvement

| Scenario | Current Design | Recommendation |
|----------|----------------|----------------|
| Many symbols | Broadcasts all updates | Add throttling (Issue #2) |
| Many clients | Sequential broadcast | Add queues (Issue #4) |
| API failures | No protection | Add circuit breaker (Issue #1) |
| Long sessions | Unbounded cache | Add TTL eviction (Issue #5) |

### Error Recovery and Resilience: ‚ö†Ô∏è Needs Improvement

| Failure Mode | Current Handling | Recommendation |
|--------------|------------------|----------------|
| Alpaca API down | Exception bubbles up | Circuit breaker + fallback |
| WebSocket disconnect | Frontend reconnection mentioned | Add reconnection with backoff |
| Price stream failure | Logged, no recovery | Auto-reconnect with watchdog |
| Initial fetch failure | Single attempt | Retry with exponential backoff |

---

## Plan Compliance Check

| Requirement | Status | Notes |
|-------------|--------|-------|
| REST for initial positions | ‚úÖ | GET /api/positions endpoint defined |
| WebSocket for price streaming | ‚úÖ | option_price_update event type defined |
| OCC symbol parsing | ‚úÖ | OCCSymbol.parse() implemented with regex |
| Iron condor detection | ‚úÖ | 4-leg grouping logic in _group_into_iron_condors |
| P/L calculation | ‚úÖ | Both backend (computed_field) and frontend |
| Error handling | ‚ö†Ô∏è | Basic try/except, needs circuit breaker |
| Reconnection logic | ‚ö†Ô∏è | Mentioned but not detailed |
| Caching | ‚ö†Ô∏è | Price cache exists but unbounded |

---

## Positive Architecture Decisions

1. **Pydantic Models Throughout**: Consistent use of strongly-typed Pydantic models (`OCCSymbol`, `OptionLeg`, `IronCondorPosition`) ensures data validation and serialization consistency.

2. **Computed Fields for P/L**: Using Pydantic `@computed_field` for P/L calculations keeps logic centralized and ensures consistency between frontend display and backend storage.

3. **Existing Pattern Reuse**: The plan correctly extends `WebSocketManager` with new broadcast methods following the existing pattern (`broadcast_adw_event`, `broadcast_agent_created`, etc.).

4. **Composables for Frontend**: Creating `useAlpacaPositions` and `useAlpacaPriceStream` composables follows Vue 3 best practices and separates concerns from the component.

5. **Store Integration**: Integrating with `orchestratorStore` for global state management ensures the Alpaca data participates in the existing reactive system.

---

## Verification Checklist

- [x] REST + WebSocket hybrid pattern is appropriate
- [x] Data models are well-defined with Pydantic
- [x] Separation of concerns is maintained
- [ ] Circuit breaker pattern for external API
- [ ] Rate limiting for high-frequency updates
- [ ] Backpressure handling for WebSocket broadcasts
- [ ] Cache eviction strategy
- [ ] Reconnection with exponential backoff
- [x] Unit tests planned for OCC parser
- [ ] Integration tests for full data flow

---

## Final Verdict

**Status**: ‚úÖ PASS (with recommendations)

**Reasoning**: The architecture plan is fundamentally sound and follows established patterns in the codebase. The hybrid REST + WebSocket approach is correct for the use case. The plan can proceed to implementation, but the HIGH and MEDIUM risk items should be addressed either during implementation or as fast-follow improvements.

**Next Steps**:
1. **Before Implementation**: Add circuit breaker pattern design to the plan
2. **During Implementation**: Implement throttling for price updates
3. **Fast-Follow**: Add backpressure handling to WebSocketManager (benefits entire application)
4. **Testing**: Add integration tests that verify reconnection behavior

---

**Report File**: `app_review/review_2026-01-10T12-30-00Z.md`
